<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Empathic Voice Interface - Standalone</title>
    <style>
        :root {
            --gap: 1.2rem;
            --radius: 8px;
            --shadow: rgba(0, 0, 0, 0.1);
            --color-bg: #fff;
            --color-fg: #333;
            --color-muted: #666;
            --header-height: auto;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: sans-serif;
            color: var(--color-fg);
            background: #f9f9f9;
            display: flex;
            justify-content: center;
            padding: var(--gap);
            min-height: 100vh;
        }

        #app {
            width: 76%;
            max-width: 1200px;
            display: flex;
            flex-direction: column;
            gap: var(--gap);
        }

        #heading-container {
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: var(--gap);
        }

        #instructions-container {
            flex: 1 1 400px;
        }

        #instructions-container h2 {
            margin-bottom: 0.25em;
        }

        #instructions {
            line-height: 1.5;
            color: var(--color-muted);
            padding-top: 1.5rem;
        }

        #btn-container {
            display: flex;
            gap: 0.5rem;
        }

        button {
            padding: 0.6rem 1.2rem;
            font-size: 1rem;
            border: 1px solid var(--color-fg);
            border-radius: var(--radius);
            background: var(--color-bg);
            color: var(--color-fg);
            cursor: pointer;
            transition: background-color 0.2s, color 0.2s;
            width: 7rem;
        }

        button:hover:not(:disabled),
        button:focus:not(:disabled) {
            background: var(--color-fg);
            color: var(--color-bg);
        }

        button:disabled {
            background: #e0e0e0;
            color: var(--color-muted);
            border-color: #aaa;
            cursor: not-allowed;
        }

        #chat {
            display: flex;
            flex-direction: column;
            gap: var(--gap);
            max-height: calc(100vh - 200px);
            overflow-y: auto;
            padding: 1.5rem;
        }

        .chat-card {
            background: var(--color-bg);
            border-radius: var(--radius);
            padding: 1rem;
            box-shadow: 0 4px 12px var(--shadow);
            position: relative;
            width: 600px;
        }

        .chat-card.assistant {
            align-self: flex-start;
        }

        .chat-card.user {
            align-self: flex-end;
        }

        .chat-card .role {
            font-weight: bold;
            margin-bottom: 0.5rem;
        }

        .chat-card .timestamp {
            position: absolute;
            top: 0.8rem;
            right: 0.8rem;
            font-size: 0.75rem;
            color: var(--color-muted);
        }

        .chat-card .content {
            margin-bottom: 0.75rem;
        }

        .chat-card .scores {
            display: flex;
            gap: 1rem;
            font-size: 0.8rem;
            color: var(--color-muted);
        }
    </style>
</head>
<body>
    <main id="app">
        <header id="heading-container">
            <div id="instructions-container">
                <h1>EVI TypeScript Quickstart</h1>
                <p id="instructions">
                    Click <strong>Start</strong> to connect, grant mic access, then speak.
                    Click <strong>Stop</strong> to end the session. <br>
                    ‚öôÔ∏è Open your browser console to see socket logs and errors.<br>
                    üîë Pass API key/token and config via URL: ?apiKey=YOUR_KEY&configId=YOUR_CONFIG or ?accessToken=YOUR_TOKEN&configId=YOUR_CONFIG
                </p>
            </div>
            <div id="btn-container">
                <button id="start-btn">Start</button>
                <button id="stop-btn" disabled="true">Stop</button>
            </div>
        </header>

        <section id="chat"></section>
    </main>

    <!-- Load Hume SDK from CDN -->
    <script type="module">
        import * as Hume from 'https://esm.sh/hume@0.12.1';
        
        (async () => {
            // Parse URL parameters
            const urlParams = new URLSearchParams(window.location.search);
            const apiKey = urlParams.get('apiKey') || urlParams.get('api_key');
            const accessToken = urlParams.get('accessToken') || urlParams.get('access_token');
            const configId = urlParams.get('configId') || urlParams.get('config_id') || urlParams.get('config') || null;

            const startBtn = document.querySelector('#start-btn');
            const stopBtn = document.querySelector('#stop-btn');
            const chatContainer = document.querySelector('#chat');

            let socket = null;
            let recorder = null;
            let player = new Hume.EVIWebAudioPlayer();

            function setConnected(on) {
                if (startBtn) startBtn.disabled = on;
                if (stopBtn) stopBtn.disabled = !on;
            }

            // Audio capture functionality
            async function startAudioCapture(socket, timeSliceMs = 80) {
                const mimeTypeResult = Hume.getBrowserSupportedMimeType();
                const mimeType = mimeTypeResult.success ? mimeTypeResult.mimeType : Hume.MimeType.WEBM;

                const micAudioStream = await Hume.getAudioStream();
                Hume.ensureSingleValidAudioTrack(micAudioStream);

                const recorder = new MediaRecorder(micAudioStream, { mimeType });
                recorder.ondataavailable = async (e) => {
                    if (e.data.size > 0 && socket.readyState === WebSocket.OPEN) {
                        const data = await Hume.convertBlobToBase64(e.data);
                        socket.sendAudioInput({ data });
                    }
                };
                recorder.onerror = (e) => console.error("MediaRecorder error:", e);
                recorder.start(timeSliceMs);

                return recorder;
            }

            // UI functionality
            function extractTopThreeEmotions(message) {
                const scores = message.models.prosody?.scores;
                const scoresArray = Object.entries(scores || {});

                scoresArray.sort((a, b) => b[1] - a[1]);

                const topThreeEmotions = scoresArray.slice(0, 3).map(([emotion, score]) => ({
                    emotion,
                    score: Number(score).toFixed(2),
                }));

                return topThreeEmotions;
            }

            function appendChatMessage(container, msg) {
                if (!container || !msg) return;

                const { role, content } = msg.message;
                const timestamp = new Date().toLocaleTimeString();

                const card = document.createElement("div");
                card.className = `chat-card ${role}`;

                card.innerHTML = `
                    <div class="role">${role[0].toUpperCase() + role.slice(1)}</div>
                    <div class="timestamp"><strong>${timestamp}</strong></div>
                    <div class="content">${content}</div>
                `;

                const scoresEl = document.createElement("div");
                scoresEl.className = "scores";

                const topEmotions = extractTopThreeEmotions(msg);
                topEmotions.forEach(({ emotion, score }) => {
                    const item = document.createElement("div");
                    item.className = "score-item";
                    item.innerHTML = `${emotion}: <strong>${score}</strong>`;
                    scoresEl.appendChild(item);
                });

                card.appendChild(scoresEl);
                container.appendChild(card);

                container.scrollTop = container.scrollHeight;
            }

            // EVI connection functionality
            let client = null;

            function getClient() {
                if (!client) {
                    const config = accessToken ? { accessToken } : { apiKey };
                    client = new Hume.HumeClient(config);
                }
                return client;
            }

            function connectEVI(handlers, configId) {
                if (!apiKey && !accessToken) {
                    throw new Error("API key or access token is required.");
                }

                const client = getClient();
                const socket = client.empathicVoice.chat.connect({ configId });

                socket.on("open", handlers.open);
                socket.on("message", handlers.message);
                socket.on("error", handlers.error);
                socket.on("close", handlers.close);

                return socket;
            }

            // Event handlers
            async function handleOpen() {
                console.log("Socket opened");
                recorder = await startAudioCapture(socket);
                await player.init();
            }

            async function handleMessage(msg) {
                switch (msg.type) {
                    case "chat_metadata":
                        console.log(msg);
                        break;
                    case "user_message":
                    case "assistant_message":
                        if (msg.type === "user_message") {
                            player.stop();
                        }
                        appendChatMessage(chatContainer, msg);
                        break;
                    case "audio_output":
                        await player.enqueue(msg);
                        break;
                    case "user_interruption":
                        console.log("User interruption detected.");
                        player.stop();
                        break;
                    case "error":
                        console.error(`EVI Error: Code=${msg.code}, Slug=${msg.slug}, Message=${msg.message}`);
                        break;
                }
            }

            function handleError(err) {
                console.error("Socket error:", err);
            }

            function handleClose(e) {
                console.log("Socket closed:", e);
                disconnect();
            }

            function connect() {
                if (socket && socket?.readyState < WebSocket.CLOSING) return;
                setConnected(true);

                try {
                    const handlers = {
                        open: handleOpen,
                        message: handleMessage,
                        error: handleError,
                        close: handleClose,
                    };

                    socket = connectEVI(handlers, configId);
                } catch (err) {
                    console.error("Failed to connect EVI:", err);
                    socket = null;
                    setConnected(false);
                }
            }

            function disconnect() {
                if (socket && socket.readyState < WebSocket.CLOSING) socket.close();
                socket = null;

                recorder?.stream.getTracks().forEach((t) => t.stop());
                recorder = null;

                player?.dispose();

                setConnected(false);
            }

            // Event listeners
            startBtn?.addEventListener("click", connect);
            stopBtn?.addEventListener("click", disconnect);
            setConnected(false);
        })();
    </script>
</body>
</html>
